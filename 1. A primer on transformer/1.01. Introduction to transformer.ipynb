{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APrimer on Transformer\n",
    "\n",
    "A transformer is one of the most popular state-of-the-art deep learning architectures which is majorly used for NLP tasks. Ever since the advent of the transformer, it has replaced RNN and LSTM for various tasks. Several new NLP models such as BERT, GPT, and T5 are based on transformer architecture. In this chapter, we will look into the transformer in detail and understand how they work. \n",
    "\n",
    "We will begin the chapter by getting the basic idea of the transformer. Then we learn how the transformer uses encoder-decoder architecture for a language translation task. Following this, we will inspect how the encoder of the transformer works in detail by exploring each of the encoder components. After understanding the encoder, we will deep dive into the decoder and look into each of the decoder components in detail. By the end of the chapter, we put the encoder and decoder together and see how the transformer works as a whole. \n",
    "\n",
    "In this chapter, we will learn the following topics: \n",
    "\n",
    "- Introduction to the transformer\n",
    "- Understanding the encoder of transformer\n",
    "- Understanding the decoder of transformer\n",
    "- Putting encoder and decoder together\n",
    "- Training the transformer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the transformer \n",
    "\n",
    "RNN and LSTM networks are widely used in sequential tasks such as next word prediction, machine translation, text generation, and more. However one of the major challenges with the recurrent model is capturing the long-term dependency. This is because RNN is sequential in nature which in turn inhibits parallelization.\n",
    "\n",
    "To overcome the limitation of RNN, a new architecture called Transformer is introduced in the paper Attention is All you Need.  The transformer is currently the state of the art model for several natural language processing (NLP) tasks. The advent of the transformer has created a major breakthrough in the field of the NLP and also paved the way for new revolutionary architectures such as BERT, GPT-2, T5, and more. \n",
    "\n",
    "The transformer model is based entirely on the attention mechanism and completely gets rid of recurrence.  The transformer uses a special type of attention mechanism called a self-attention. We will learn about this in detail in the upcoming sections. \n",
    "\n",
    "Let’s understand how the transformer works with a language translation task. The transformer consists of an encoder-decoder architecture. We feed the input sentence (source sentence) to the encoder. The encoder learns the representation of the input sentence and sends the representation to the decoder. The decoder receives the representation learned by the encoder as an input and generates the output sentence (target sentence). \n",
    "\n",
    "Suppose, we need to convert a sentence from English to French. As shown in the following figure, we feed the English sentence as an input to the encoder. The encoder learns the representation of the given English sentence and feeds the representation to the decoder. The decoder takes the encoder's representation as an input and generates the French sentence as an output: \n",
    "\n",
    "\n",
    "![title](images/1.png)\n",
    "\n",
    "Okay, but what’s exactly going on here? How does the encoder and decoder in the transformer convert the English sentence (source sentence) to the French sentence (target sentence)? What's going on inside the encoder and decoder? Let us find this out in the next sections. First, we will look into the encoder in detail, and then we will see the decoder. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
