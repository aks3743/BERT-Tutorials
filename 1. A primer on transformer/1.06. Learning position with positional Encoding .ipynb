{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning position with positional Encoding \n",
    "\n",
    "Consider the input sentence - 'I am good' . In RNN, we feed the sentence to the network word by word. That is, first the word ‘I’ is passed as an input, next, the word 'am' is passed, and so on. We feed the sentence, word by word so that our network understands the sentence completely. But with the transformer network, we don’t follow the recurrence mechanism, instead of feeding the sentence, word by word, we feed all the words in the sentence parallel to the network. Feeding the words in parallel helps in decreasing the training time and also it helps in learning the long-term dependency.\n",
    "\n",
    "However, the problem is since we feed the words parallel to the transformer, how will it understand the meaning of the sentence if the word order is not retained? \n",
    "\n",
    "To understand the sentence, the word order (position of the words in the sentence) is important, right? Yes, the word order is very important as it helps to understand the position of each word in a sentence which in turn helps to understand the meaning of the sentence. \n",
    "\n",
    "So, we should give some information about the word order to the transformer so that it can understand the sentence. How can we do that? Let’s explore more about this in detail now. \n",
    "\n",
    "\n",
    "For our given sentence - 'I am good', first, we get the embeddings for each word in our sentence. Let's represent the embedding dimension by $d_{\\text{model}}$. Say the embedding dimension, $d_{\\text{model}}$ is 4. Then our input matrix dimension will be [sentence length x embedding dimension] = [3x4]. \n",
    "\n",
    "We represent our input sentence 'I am good' using the input matrix $X$ (embedding matrix). Let the input matrix $X$ be the following : \n",
    "\n",
    "\n",
    "![title](images/25.png)\n",
    "\n",
    "\n",
    "Now, if we pass the preceding input matrix $X$ directly to the transformer, it cannot understand the word order. So, instead of feeding the input matrix directly to the transformer. We need to add some information indicating the word order (position of the word) so that our network can understand the meaning of the sentence. To do this, we introduce a technique called positional encoding. Positional encoding as the name suggests is the encoding indicating the position of the word in a sentence (word order). \n",
    "\n",
    "The dimension of the positional encoding matrix $P$ is the same dimension as the input matrix $X$. Now, before feeding the input matrix (embedding matrix) to the transformer directly, we include the positional encoding. So, we simply add the positional encoding matrix $P$ with the embedding matrix $X$ and then feed them as an input to the network.  So, now our input matrix will not only have the embedding of the word but also the position of the word in the sentence:\n",
    "\n",
    "\n",
    "![title](images/26.png)\n",
    "\n",
    "Now, the ultimate question is how exactly the positional encoding matrix is computed? The authors of the transformer paper \"Attention is all you need\" have used the sinusoidal function for computing the positional encoding as shown below:\n",
    "\n",
    "$$P{(\\text{pos},2i)} =\\text{sin} \\bigg(\\frac{\\text{pos}}{1000^{2i/d_{\\text{model}}}}\\bigg) $$\n",
    "\n",
    "$$ P{(\\text{pos},2i+1)} =\\text{cos} \\bigg(\\frac{\\text{pos}}{1000^{2i/d_{\\text{model}}}}\\bigg) $$\n",
    "\n",
    "In the preceding equation, $\\text{pos}$ implies the position of the word in a sentence, and  $i$ implies the position of the embedding. Let’s understand the preceding equations with an example. By using the above equations, we can write:\n",
    "\n",
    "\n",
    "![title](images/27.png)\n",
    "\n",
    "As we can observe from the preceding matrix, in the positional encoding, we use sin function when $i$ is even and cos function when $i$ is odd. Simplifying the preceding matrix, we can write:\n",
    "\n",
    "![title](images/28.png)\n",
    "\n",
    "We know that in our input sentence the word 'I' is at 0th position, 'am' is at 1st position, and 'good ' is 2nd position. Substituting the $\\text{pos}$  value, we can write:\n",
    "\n",
    "![title](images/29.png)\n",
    "\n",
    "Thus, our final positional encoding matrix $P$ is given as:\n",
    "\n",
    "\n",
    "![title](images/30.png)\n",
    "After computing the positional encoding , we simply perform element-wise addition with the embedding matrix  and feed the modified input matrix to the encoder. \n",
    "\n",
    "Now, let's revisit our encoder architecture. A single encoder block is shown in the following figure. As we can observe, before feeding the input directly to the encoder, first, we get the input embedding (embedding matrix) and then we add the positional encoding to it and then we feed them as an input to the encoder: \n",
    "\n",
    "![title](images/31.png)\n",
    "\n",
    "We learned how positional encoder works, we also learned how the sub-layer, multi-head attention works in the previous section. In the next section, we will learn how the sub-layer, feedforward network works in the encoder.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
