{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention mechanism \n",
    "Let us understand the self-attention mechanism with an example. Consider the following sentence:\n",
    "\n",
    "‘A dog ate the food because it was hungry’\n",
    "\n",
    "In the preceding sentence, the pronoun 'it' could mean either 'dog' or 'food'. By reading the sentence we can easily understand that the pronoun 'it' implies the 'dog' and not 'food'. But how our model understands that in the given sentence, the pronoun 'it' implies the 'dog' and not 'food'? Here is where the self-attention mechanism helps us.\n",
    "\n",
    "In the given sentence, ‘A dog ate the food because it was hungry’. First, our model computes the representation of the word 'A', next it computes the representation of the word 'dog', next it computes the representation of the word ‘ate’, and so on. While computing the representation of each word, it relates each word to all other words in the sentence to understand more about the word. \n",
    "\n",
    "For instance, while computing the representation of the word ‘it’, our model relates the word ‘it’ to all the words in the sentence to understand more about the word ‘it’.\n",
    "\n",
    "As shown in the following figure, in order to compute the representation of the word ‘it’ our model relates the word 'it' to all the words in the sentence. By relating the word 'it' to all the words in the sentence, our model can understand that the word 'it' is related to the word 'dog' and not 'food'. As we can observe, the line connecting the word 'it' to 'dog' is solid compared to other lines which indicate that the word 'it' is related to the word 'dog' and not 'food' in the given sentence:\n",
    "\n",
    "![title](images/5.png)\n",
    "\n",
    "Okay, but how exactly does this work? Now that we have a basic idea of what the self-attention mechanism is, let’s understand more about them in detail. \n",
    "\n",
    "Suppose, our input sentence (source sentence) is 'I am good'. First, we get the embeddings for each word in our sentence. Note that the embeddings are just the vector representation of the word and the values of the embeddings will be learned during training. \n",
    "\n",
    "\n",
    "Let $x_1$ be the embedding of the word 'I', $x_2$ be the embedding of the word 'am' and $x_3$ be the embedding of the word 'good'. Consider the following:\n",
    "\n",
    "- Embedding of the word 'I' be $x_1 = [1.79, 2.22, \\dots, 6.66] $\n",
    "- Embedding of the word 'am' be $x_2 = [7.77, 0.631, \\dots, 5.35] $\n",
    "- Embedding of the word 'good' be $x_3 = [11.44, 10.10, \\dots, 3.33] $\n",
    "\n",
    "Then we can represent our input sentence 'I am good' using the input matrix (embedding matrix or input embedding) as shown below: \n",
    "\n",
    "![title](images/6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note that the values used in the preceding matrix are arbitrary just to give us a better understanding.**\n",
    "\n",
    "From the preceding input matrix, $X$, we can understand that the first row of the matrix implies the embedding of the word 'I', the second row implies the embedding of the word 'am' and the third row implies the embedding of the word 'good'. Thus, the dimension of the input matrix $X$ will be [sentence length  X embedding dimension].  The number of words in our sentence (sentence length) is 3. Let the embedding dimension be 512, then our input matrix (input embedding) dimension will be [3 x 512].\n",
    "\n",
    "Now, from the input matrix $X$, we create three new matrices called a query matrix, $Q$, key matrix $K$, and value matrix $V$.  Wait. What are these three new matrices? And why do we need them? They are used in the self-attention mechanism. We will see how exactly these three matrices are used in a while. \n",
    "\n",
    "Okay, how we can create the query, key, and value matrices? To create these, we introduce three new weight matrices called $W_Q$,$W_K$, $W_V$. We create the query $Q$, key $K$, and value $V$ matrices, by multiplying the input matrix  $X$ by  $W_Q$,$W_K$, $W_V$  respectively. \n",
    "\n",
    "Note that the weight matrices $W_Q$,$W_K$, $W_V$ are randomly initialized and their optimal values will be learned during training. As we learn the optimal weights, we will obtain more accurate query, key, and value matrices. \n",
    "\n",
    "As shown in the following figure, multiplying the input matrix $X$ by the weight matrices $W_Q$,$W_K$, $W_V$ , we obtain the query, key, and value matrices:\n",
    "\n",
    "![title](images/7.png)\n",
    "\n",
    "From the preceding image, we can understand the following:\n",
    "\n",
    "- The first row in the query, key, and value matrix - $q_1$, $k_1$, $v_1$ implies the query, key, and value vector of the word 'I'. \n",
    "- The second row in the query, key, and value matrix- $q_2$, $k_2$, $v_2$ implies the query, key, and value vector of the word 'am'. \n",
    "- The third row in the query, key, and value matrix- $q_3$, $k_3$, $v_3$ implies the query, key, and value vector of the word 'good'.\n",
    "\n",
    "Note that the dimensionality of the query, key, value vectors are 64. Thus, the dimension of our query, key, and value matrices are [sentence length x 64]. Since we have three words in the sentence, the dimension of the query, key, and value matrices are [3x64]\n",
    "\n",
    "But still, the ultimate question is why are we computing this? What is the use of query, key, and value matrices? How this is going to help us? This is exactly what we discuss in detail in the next section. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
