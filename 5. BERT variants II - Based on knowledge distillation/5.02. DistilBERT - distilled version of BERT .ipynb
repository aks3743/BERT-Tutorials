{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT - distilled version of BERT \n",
    " The pre-trained BERT model has a large number of parameters and also high inference time which makes it harder to use them on edge devices like mobile phones. To solve this issue, we use DistilBERT which was introduced by researchers at Hugging Face. DistilBERT is the smaller, faster, cheaper, and lighter version of BERT.\n",
    "\n",
    "As the name suggests DistilBERT uses knowledge distillation. The ultimate idea of DistilBERT is that we take a large pre-trained BERT model and transfer its knowledge to a small BERT through knowledge distillation. The large pre-trained BERT is called a teacher BERT and the small BERT is called a student BERT.  \n",
    "\n",
    "Since the small BERT (student BERT) acquires its knowledge from the large pre-trained BERT (teacher BERT) through distillation, we can call our small BERT the DistilBERT. The DistilBERT is 60% faster and its size is 40% less compared to the large BERT models. Now that we have a basic idea of DistilBERT, let us get into details and learn how they work. \n",
    "\n",
    "## Teacher-student architecture \n",
    "Let's begin by understanding the architecture of teacher and student BERT in detail. First, we look into the teacher BERT, then we look into the student BERT. \n",
    "\n",
    "# Teacher BERT\n",
    "The teacher BERT is a large pre-trained BERT model. We use the pre-trained BERT-base model as the teacher. We have already learned how the BERT-base model is pre-trained in the previous chapters. We know that the BERT model is pre-trained using the masked language modeling and next sentence prediction task. \n",
    "\n",
    "Since the BERT is pre-trained using the masked language modeling task, we can use our pre-trained BERT model for predicting the masked word. The pre-trained BERT-base model is shown in the following figure:\n",
    "\n",
    "\n",
    "![title](images/9.png)\n",
    "\n",
    "From the preceding figure, we can notice that given a masked input sentence, our pre-trained BERT gives the probability distribution of all the words in the vocabulary to be the masked word. This probability distribution contains the dark knowledge and we need to transfer this knowledge to the student BERT. Let us see how we do that in the upcoming sections. \n",
    "\n",
    "## Student BERT\n",
    "Unlike teacher BERT, the student BERT is not pre-trained. The student BERT has to learn from the teacher. The student BERT is a small BERT and it contains a lesser number of layers compared to the teacher BERT. The teacher BERT consists of 110M parameter but the student BERT consist of only 66M parameter. \n",
    "\n",
    "Since we have less number of layers in the student BERT, it helps us train it faster compared to the teacher BERT (BERT-base). \n",
    "\n",
    "The researchers of DistilBERT have kept the hidden state dimension of student BERT to 768 the same as what we have in the teacher BERT (BERT-base). They have observed that reducing the hidden state dimension has not significantly affected the computation efficiency. So, they have focused on only reducing the number of layers. \n",
    "\n",
    "Okay. How to train the student BERT? Now that we have understood the architecture of student BERT, let us learn how to train the student BERT by distilling knowledge from the teacher BERT in the next section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
