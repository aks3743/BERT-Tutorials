{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge distillation \n",
    "Knowledge distillation is a model compression technique in which a small model is trained to reproduce the behavior of a large pre-trained model. It is also referred to as teacher-student learning where the large pre-trained model is the teacher and the small model is the student. Let us understand how knowledge distillation works with an example. \n",
    "\n",
    "Suppose we have pre-trained a large model to predict the next word in a sentence. We call this large pre-trained model a teacher network. If we feed a sentence and let the network predict the next word in the sentence then it will return the probability distribution over all the words in the vocabulary to be the next word as shown in the following figure. Note that for simplicity and better understanding, let us assume, we have only five words in our vocabulary:\n",
    "\n",
    "\n",
    "![title](images/1.png)\n",
    "\n",
    "From the preceding figure, we can observe the probability distribution returned by the network. This probability distribution is essentially obtained by applying the softmax function in the output layer and we select the word which has the high probability as the next word in the sentence. Since the word Homework has a high probability, we select the next word in the sentence as Homework.\n",
    "\n",
    "Apart from selecting the word which has a high probability, can we extract some other useful information from the probability distribution returned by our network? Yes! From the following figure, we can observe that apart from the word which has the highest probability there are some words that have high probability compared to other words. That is, as we can see, the words Book and Assignment have slightly high probability compared to other words like Cake and Car as shown in the following figure: \n",
    "\n",
    "\n",
    "![title](images/2.png)\n",
    "\n",
    "This indicates that apart from the word homework, the words book and assignment are more relevant to the given sentence compared to words like cake, and car. This is known as dark knowledge. During the knowledge distillation, we want our student network to learn this dark knowledge from the teacher. \n",
    "\n",
    "Okay, but usually, any good model will return a high probability close to 1 for the correct class and probabilities very close to 0 for other classes right? Yes! For instance, considering the same example we saw earlier, let's suppose, our model has returned the following probability distribution:\n",
    "\n",
    "\n",
    "![title](images/3.png)\n",
    "\n",
    "From the preceding figure, we can notice that our model has returned a very high probability to the word homework and probability so close to 0 for all other words. We can observe that we don't have much information in the probability distribution apart from the ground truth (correct word). So, how we can extract dark knowledge here?\n",
    "\n",
    "In this case, we use the softmax function with a temperature. It is commonly referred to as softmax temperature. We use the softmax temperature in the output layer. It is used to smoothen the probability distribution. The softmax function with temperature is expressed as:\n",
    "\n",
    "$$ P_i = \\frac{\\text{exp}(z_i/T)}{\\sum_j \\text{exp}(z_j/T)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding equation $T$ is the temperature. When we set $T=1$, then it is just our standard softmax function. Increasing the value of $T$ , makes the probability distribution softer and gives more information about other classes.\n",
    "\n",
    "For example, as shown in the following figure, when $T=1$, we have the same probability distribution returned by our network with standard softmax function. When $T=2$, the probability distribution is smoothened and with $T=5$, the probability distribution is further smoothened. So, by increasing the value of  $T$, we get smoothened probability distribution which gives more information about other classes: \n",
    "\n",
    "\n",
    "![title](images/4.png)\n",
    "\n",
    "\n",
    "Thus, with softmax temperature, we can obtain the dark knowledge. First, we pre-train the teacher network with softmax temperature to obtain dark knowledge. Then, during knowledge distillation (transferring knowledge from the teacher to the student), we transfer this dark knowledge from the teacher to the student.\n",
    "\n",
    "Okay, but how we transfer the dark knowledge from the teacher to the student? How the student network is trained and how it acquires knowledge from the teacher? This is exactly what we discuss in the next section.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
