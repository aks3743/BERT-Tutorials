{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyBERT \n",
    "TinyBERT is another interesting variant of BERT which also uses knowledge distillation. In the DistilBERT, we learned how to transfer knowledge from the output layer of the teacher BERT to the student BERT. But apart from this, can we also learn other information from teacher BERT? Yes! Apart from transferring knowledge from the output layer of the teacher to student BERT, we can also transfer knowledge from other layers.\n",
    "\n",
    "In TinyBERT, apart from transferring knowledge from the output layer (prediction layer) of the teacher to the student, we also transfer knowledge from embedding and encoder layers. \n",
    "\n",
    "Let us understand this with an example. Suppose we have a teacher BERT with N encoder layers. For simplicity, we have shown only 1 encoder layer in the following figure. The following figure depicts the pre-trained teacher BERT model where we feed the masked sentence to BERT and it returns the logits of all the words in our vocabulary to be the masked word. \n",
    "\n",
    "In the distill BERT, (1) we took the logits produced by the output layer of the teacher BERT and trained the student BERT to produce the same logits. Apart from this now in TinyBERT, we also take the (2) hidden state and attention matrix produced by the teacher BERT and train the student BERT to produce the same hidden state and attention matrix. Next, we also take the (3) output of the embedding layer from the teacher BERT and train the student BERT to produce the same embedding as the teacher BERT:\n",
    "\n",
    "\n",
    "![title](images/12.png)\n",
    "\n",
    "Thus, in TinyBERT, apart from transferring knowledge from the output layer of the teacher BERT to the student BERT, we also transfer knowledge from the intermediate layers. Transferring knowledge of intermediate layers from teacher to student BERT helps the student BERT to learn more information from the teacher. For instance, the attention matrix encapsulates linguistic information, and thus transferring the knowledge from the attention matrix of the teacher to the student, helps the student BERT to learn linguistic information from the teacher. \n",
    "\n",
    "Apart from this, In TinyBERT we use a two-stage learning framework where we apply distillation in both the pre-training and fine-tuning stage. We will learn how exactly this two-stage learning helps us as we go through the next sections. Now that we got a basic idea and overview of TinyBERT, let's explore more about them in detail. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
