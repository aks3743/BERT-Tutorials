{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring knowledge from BERT to Neural Networks\n",
    "In this section, let's look into an interesting paper Distilling Task-Specific Knowledge from BERT into Simple Neural Networks by the University of Waterloo. In this paper, the authors have explained how to perform knowledge distillation and transfer task-specific knowledge from the BERT to a simple neural network. Let's get into details and understand how exactly this works. \n",
    "\n",
    "## Teacher-student architecture \n",
    "To understand how exactly we transfer task-specific knowledge from BERT to a neural network, first let us take a look into the teacher BERT and student network in detail. \n",
    "\n",
    "### Teacher BERT \n",
    "We use the pre-trained BERT as the teacher BERT. Here, we use the pre-trained BERT-large as the teacher BERT. Note that here we are transferring task-specific knowledge from the teacher to the student. So, first, we take the pre-trained BERT-large model, fine-tune it for a specific task, and then use it as the teacher. \n",
    "\n",
    "Suppose, we want to train our student network for the sentiment analysis task. In that case, we take the pre-trained BERT-large model, fine-tune it for a sentiment analysis task, and then use it as the teacher. Thus our teacher is the pre-trained BERT fine-tuned for a specific task of our interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student network \n",
    "The student network is a simple bidirectional LSTM and we can shortly represent this as BiLSTM. The architecture of the student network changes based on the task. Let's take into the architecture of the student network for a single sentence classification task. \n",
    "\n",
    "Suppose we are performing sentiment analysis. Say we have a sentence: I love Paris. First, we get the embeddings of the sentence, then we feed the input embeddings to the bidirectional LSTM. Bidirectional LSTM reads the sentence in both directions (that is, forward and backward) so we obtain the forward and backward hidden states from Bidirectional LSTM.\n",
    "\n",
    "Next, we feed the forward and backward hidden states to the fully connected layer with ReLU activation which then returns the logits as an output. We take the logits and feed them to the softmax function and obtain the probabilities of sentence belonging to positive and negative class as shown in the image:\n",
    "\n",
    "\n",
    "![title](images/18.png)\n",
    "Now, let's take a look at the architecture of the student network for sentence matching tasks. Suppose we want to understand whether the given two sentences are similar. In this case, our student network is the siamese BiLSTM. \n",
    "\n",
    "First, we get the embeddings of sentence 1 and sentence 2 and feed them to the bidirectional LSTM 1 (BiLSTM 1) and bidirectional LSTM 2 (BiLSTM 2) respectively. We obtain the forward and backward hidden states from BiLSTM 1 and BiLSTM 2.  Let $h_1$ be the forward and backward hidden state obtained from BiLSTM 1 and  $h_2$ be the forward and backward hidden state obtained from BiLSTM 2 . We combine $h_1$ and $h_2$  using  concatenateâ€“compare operation as given in the following equation:\n",
    "\n",
    "$$f(h_{s1}, h_{s2}) = \\big[ h_{s1}, h_{s2} , h_{s1} \\odot h_{s2} ,| h_{s1}- h_{s2} | \\big] $$\n",
    "\n",
    "In the preceding equation  denotes the element-wise multiplication. Next, we feed the concatenated result to the fully connected layer with ReLU activations and get return the logits. Then we feed the logits to the softmax function which returns the probabilities of the given sentence pair being similar and dissimilar as shown in the following figure:\n",
    "\n",
    "\n",
    "\n",
    "![title](images/19.png)\n",
    "\n",
    "Now that we have understood the architecture of the student network, let us see how to train the student network by acquiring knowledge from the teacher BERT in the next section. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
