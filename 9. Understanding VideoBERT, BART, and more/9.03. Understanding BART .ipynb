{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding BART \n",
    "BART is another interesting model introduced by Facebook AI. It is based on transformer architecture. The BART model is essentially a denoising autoencoder. It is trained by reconstructing the corrupted text. \n",
    "\n",
    "Just like the BERT model, we can use the pre-trained BART model and fine-tune it for several downstream tasks. The BART model is best suitable for the text generation task. It is also used for other tasks such as language translation and comprehension. The researchers have also shown that the performance of BART is equivalent to the RoBERTa model. But how exactly do BART works? What's special about BART? How it differs from BERT? Let us find out the answers to all these questions in the next section. \n",
    "\n",
    "## Architecture of BART \n",
    "The BART is essentially a transformer model with an encoder and decoder. We feed the corrupted text to the encoder and the encoder learns the representation of the given text and sends the representation to the decoder. The decoder takes the representation produced by the encoder and reconstructs the original uncorrupted text. \n",
    "\n",
    "The encoder of the BART model is bidirectional meaning that it can read the sentence in both directions (left-to-right and right-to-left) but the decoder of the BART model is unidirectional and it reads the sentence in the only left-to-right direction. Thus, in BART, we have a bidirectional encoder (both directions) and an autoregressive decoder (single direction).  \n",
    "\n",
    "The below figure shows the BART model. As we can notice, we corrupt the original text (by masking few tokens) and feed it to the encoder. The encoder learns the representation of the given text and sends the representation to the decoder which then reconstructs the original uncorrupted text:\n",
    "\n",
    "![title](images/11.png)\n",
    "\n",
    "The BART model is trained by minimizing the reconstruction loss, that is, a cross-entropy loss between the original text and the text produced by the decoder. Note that the BART is different from the BERT model. In the BERT, we just feed the masked tokens to the encoder and then feed the result of the encoder to the feedforward network which predicts the masked token. But in BART, we feed the result of the encoder to the decoder which generates/reconstructs the original sentence.\n",
    "\n",
    "The researchers have experimented with two different configurations of the BART model:\n",
    "\n",
    "- BART-base: six encoder and decoder layers\n",
    "- BART-large: twelve encoder and decoder layers\n",
    "\n",
    "We learned that we corrupt the text and feed it to the encoder of BART. How exactly we corrupt the text? Is corrupting only include masking few tokens? Not necessarily. The researchers have proposed several interesting noising techniques for corrupting the text, we will look into those techniques in the next section. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
