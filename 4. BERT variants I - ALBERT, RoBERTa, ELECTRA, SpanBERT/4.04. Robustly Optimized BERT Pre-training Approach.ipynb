{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustly Optimized BERT Pre-training Approach\n",
    "RoBERTa (Robustly Optimized BERT Pre-training Approach) is another interesting and popular variant of BERT. The researchers have observed that the BERT is severely undertrained and proposed several approaches to pre-train the BERT model. The RoBERTa is essentially the BERT with the following changes in Pre-training:  \n",
    "\n",
    "- Use dynamic masking instead of static masking in MLM task\n",
    "- Remove the next sentence prediction task and train using only MLM task\n",
    "- Train with large batch size \n",
    "- Use byte-level BPE as a tokenizer \n",
    "\n",
    "Now, let's get into details and discuss each of the preceding points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dynamic masking instead of static masking \n",
    "We learned that we pre-train the BERT using masked language modeling and the next sentence prediction task. In the masked language modeling task, we randomly mask 15% of the token and let the network predict the masked token.\n",
    "\n",
    "For instance, we have a sentence - 'We arrived at the airport in time'. Now after tokenizing and adding [CLS] and [SEP] tokens, we have: \n",
    "\n",
    "tokens = [ [CLS], we, arrived, at, the, airport, in, time, [SEP] ]\n",
    "\n",
    "Next, we randomly mask 15% of the tokens:\n",
    "\n",
    "tokens = [ [CLS], we, [MASK], at, the airport, in, [MASK], [SEP] ]\n",
    "\n",
    "Now, we feed the tokens to the BERT and train it to predict the masked tokens. Note that the masking is done only once during the pre-processing step and we train the model over several epochs to predict the same masked token. This is known as static masking. \n",
    "\n",
    "RoBERTa uses dynamic masking instead of static masking. Let us understand how dynamic masking works with an example. \n",
    "\n",
    "First, we duplicate a sentence 10 times. Say, we have 10 duplicates of the given sentence - 'We arrived at the airport in time'. Next, we randomly mask 15% of tokens in all these 10 duplicates of the sentence. So, now we have 10 sentences with different tokens masked in each sentence as shown next:\n",
    "\n",
    "\n",
    "![title](images/3.png)\n",
    "\n",
    "We train the model for 40 epochs. For every epoch, we feed the sentence with different tokens masked. For epoch 1, we feed the sentence 1, for epoch 2, we feed the sentence 2, and so on as shown below: \n",
    "\n",
    "\n",
    "![title](images/4.png)\n",
    "\n",
    "Our model will see a sentence with the same mask for only 4 epochs. For example, sentence 1 will be seen in epochs 1, 11, 21, and 31. Sentence 2 will be seen in epochs 2, 12, 22, and 32. In this way, we train the RoBERTa model using dynamic masking instead of static masking. \n",
    "\n",
    "\n",
    "## Removing the next sentence prediction task\n",
    "The researchers have observed that the next sentence prediction task is not really useful for pre-training the BERT model and they pre-trained the RoBERTa only with the masked language modeling task. To better understand the importance of the next sentence prediction (NSP) task, the following experiments are conducted:\n",
    "\n",
    "- SEGMENT-PAIR + NSP: In this setting, we train the BERT with the NSP task. This is similar to how we train the vanilla BERT model and the input consists of a segment pair less than 512 tokens. \n",
    "- SENTENCE-PAIR + NSP: Here as well, we train the BERT with the NSP task and the input consists of a sentence pair which is either sampled from a contiguous portion of one document or from different documents and the input consists of less than 512 tokens. \n",
    "- FULL SENTENCES: In this setting, we train the BERT without the NSP task. Here, our input consists of a full sentence which is sampled continuously from one or more documents. The input consists of at most 512 tokens. If we reach the end of one document, then we begin sampling from the next document. \n",
    "- DOC SENTENCES: In this setting, we train the BERT without the NSP task. It is similar to FULL-SENTENCES, but here input consists of a full sentence that is sampled only from a single document. That is, if we reach the end of one document, then we will not sample from the next document. \n",
    "\n",
    "\n",
    "The researchers have pre-trained the BERT model in the preceding four settings and evaluated the model on several datasets including SQuAD, MNLI-m, SST-2, and RACE. The following table shows the F1 score of our model on the SQuAD dataset and accuracy score for MNLI-m, SST-2, and RACE:\n",
    "\n",
    "![title](images/5.png)\n",
    "\n",
    "As we can observe from the preceding results, the BERT performs better in the FULL-SENTENCES and DOC-SENTENCES setting where we trained it without the NSP task.\n",
    "\n",
    "Compared to FULL-SENTENCES and DOC-SENTENCES, the DOC-SENTENCES where we sample sentences only from a single document performs better than a FULL-SENTENCES setting where we sample sentences across documents. But in RoBERTa, we use the FULL-SENTENCES setting because the batch sizes vary in the DOC-SENTENCES.\n",
    "\n",
    "To conclude, in RoBERTa, we train the model only with masked language modeling task and not with the next sentence prediction task and the input consists of a full sentence which is sample continuously from one or more documents. The input consists of at most 512 tokens. If we reach the end of one document, then we begin sampling from the next document. \n",
    "\n",
    "## Training with more data points\n",
    "We learned that we pre-train the BERT with the Toronto book corpus and English Wikipedia dataset which accounts for a total of 16 GB. Along with these two datasets, we pre-train the RoBERTa using the CC-News (Common Crawl-News), Open WebText, Stories (subset of common crawl) datasets. \n",
    "\n",
    "Thus the RoBERTa model is pre-trained 5 datasets and the sum of the total size of these 5 datasets is 160 GB. \n",
    "\n",
    "## Training with a large batch size \n",
    "We learned that BERT is pre-trained with a batch size of 256 sequences for 1 million steps. We pre-train the RoBERTa with a larger mini-batch size. We pre-train the RoBERTa with a batch size of 8K sequences for 300K steps. We can also pre-train longer with a batch size of 8K sequences for 500K steps. \n",
    "\n",
    "But why do we have to increase the batch size? What is the advantage of training with a large batch size? Training with a larger batch size increases the speed and performance of the model. \n",
    "\n",
    "## Using byte-level BPE as a tokenizer \n",
    "We know that BERT uses the WordPiece tokenizer. We learned that WordPiece tokenizer works similar to BPE and it merges the symbol pair based on the likelihood instead of frequency. Unlike BERT, RoBERTa uses byte-level BPE (BBPE) as a tokenizer.\n",
    "\n",
    "We learned how byte-level BPE works in Chapter 2, Understanding BERT model. We learned that BBPE works very similar to BPE but instead of using character level sequence, it uses a byte-level sequence. We know that BERT uses a vocabulary of size 30K tokens but RoBERTa uses a vocabulary of size 50k tokens. Let's explore more about RoBERTa tokenizer in the next section. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
