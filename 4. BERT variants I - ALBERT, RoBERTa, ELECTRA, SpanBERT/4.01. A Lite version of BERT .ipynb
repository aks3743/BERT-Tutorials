{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Lite version of BERT \n",
    "In this section, we will learn about A Lite version of BERT shortly known as ALBERT. One of the challenges with the BERT is that it consists of millions of parameters. The BERT base consists of 110 million parameters which make it harder to train and it also has high inference time. Increasing model size gives us good results but it puts a limitation on the computational resource. To combat this, ALBERT was introduced. ALBERT is a lite version of BERT with fewer parameters compared to BERT. It uses the following two techniques to reduce the number of parameters: \n",
    "\n",
    "- Cross-layer parameter sharing\n",
    "- Factorized embedding layer parameterization \n",
    "\n",
    "By using the above two techniques, we can reduce the training time and inference time of the BERT model. First, let us understand how these two techniques work in detail, and then we will see how ALBERT is pre-trained. \n",
    "\n",
    "## Cross-layer parameter sharing \n",
    "Cross-layer parameter sharing is an interesting method for reducing the number of parameters of the BERT model. We know that the BERT consists of an N number of encoder layers. For instance, the BERT-base consists of 12 encoder layers. During training, we learn the parameters of all the encoder layers. But with cross-layer parameter sharing, instead of learning the parameters of all the encoder layers, we only learn parameters of the first encoder layer and then we just share the parameters of the first encoder layer to all the other encoder layers. Let us explore this in detail. \n",
    "\n",
    "The following figure shows the BERT model with N number of encoder layers and only the first encoder layer is expanded to reduce the clutter:\n",
    "\n",
    "\n",
    "![title](images/1.png)\n",
    "\n",
    "We know that each encoder layer is identical, that is, each encoder consists of sub-layers called multi-head attention and feedforward layers. We can just learn the parameters of encoder 1 and share the parameters with all other encoders and this known as cross-layer parameter sharing. We have several options to perform cross-layer parameter sharing as listed here: \n",
    "\n",
    "- All-shared: In all-shared, we share parameters of all the sublayers of the first encoder to all the sublayers in the other encoders. \n",
    "- Shared feedforward network: Here, we only share the parameters of the feedforward network of the first encoder layer to the feedforward network of other encoder layers. \n",
    "- Shared attention: In this option, we only share the parameters of the multi-head attention of the first encoder layer to the multi-head attention of other encoders layers.\n",
    "\n",
    "By default, ALBERT uses the all-shared option, that is, we share parameters of the first encoder layer to all the layers.\n",
    "Now that we learned how the cross-layer parameter sharing technique, let us look into another interesting parameter reduction technique in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized embedding parameterization\n",
    "In the BERT model, the WordPiece embedding size is set the same as the hidden layer embedding size (representation size). The WordPiece embedding learns the non-contextual representation and it is learned from the one-hot encoded vectors of vocabulary. The hidden layer embedding is returned by the encoder and it holds the contextual representation of the tokens (WordPieces). We set the WordPiece embedding size the same as the hidden layer embedding size because we project the one-hot encoded vectors directly to the hidden space. \n",
    "\n",
    "Let us denote the vocabulary size by V. We learned that the vocabulary size of BERT is 30K. Let us denote the hidden layer embedding size by H and the WordPiece embedding size by E. \n",
    "\n",
    "To encode more information into hidden layer embedding, we usually set the size of hidden layer embeddings to a high value. For instance, in BERT-base, the hidden layer embedding size is set to 768. The dimension of our hidden layer embedding will be V x H = 30K x 768. Since the word embedding size is set the same as the hidden layer embedding size, if the hidden layer embedding H size is 768, then the size of WordPiece embedding E size is also set to 768. The dimension of our WordPiece embedding will be V x E = 30K x 768. Thus, increasing the size of the hidden layer embedding H will also increase the size of WordPiece embedding E.\n",
    "\n",
    "Both the WordPiece embedding and hidden layer embedding are learned during training. Setting the WordPiece embedding size the same as the hidden layer embedding size increases the number of parameters to learn. So how to avoid this? We learned that we set the size of WordPiece embedding the same as the hidden layer embedding size because we directly project the one-hot encoded vectors to the hidden space.  To avoid this, we use the factorized embedding parameterization method where we factorize the embedding matrix into smaller matrices.\n",
    "\n",
    "With factorization, instead of directly projecting the one-hot encoded vectors to the hidden space (V x H), first, we project one-hot encoded vectors to low dimensional embedding space (V x E), and then we project this low dimensional embedding space to hidden space (E x H). That is, instead of projecting V x H directly, we factorize this step into V x E and E x H.\n",
    "\n",
    "Let us understand this with an example. Say, the size of our vocabulary V is 30K. Now, we don't have to set the WordPiece embedding size the same as the hidden layer embedding size. Say, the WordPiece embedding E size is 128 and the hidden layer embedding H size is 768. Now, we project V x H with the following steps: \n",
    "\n",
    "- First, we project the one-hot encoded vectors to low dimensional WordPiece embedding space. The dimension of WordPiece embedding becomes V x E = 30K x 128. \n",
    "- Next, we project this WordPiece embedding space E into a hidden layer H, that is, E x H, so the dimension becomes 128 x 768.  \n",
    "\n",
    "Thus instead of directly projecting V x H, we split it into V x E and E x H. \n",
    "\n",
    "In this way, by using cross-layer parameter sharing and factorized parameterized sharing, we can reduce our model parameters. We learned how ALBERT reduces the number of model parameters but do you know how to train the ALBERT model? Is it the same as the BERT or does it uses any different approach? Let us find that out in the next section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
