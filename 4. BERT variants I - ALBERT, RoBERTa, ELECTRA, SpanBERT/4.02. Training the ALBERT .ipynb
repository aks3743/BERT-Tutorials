{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the ALBERT \n",
    "Similar to BERT, the ALBERT model is pre-trained using the English Wikipedia and Toronto BookCorpus dataset. We learned that BERT is pre-trained using masked language modeling and the next sentence prediction tasks. Similarly, the ALBERT model is pre-trained using the masked language modeling task but instead of using the next sentence prediction task, the ALBERT uses a new task called a Sentence Order Prediction (SOP). But why not the next sentence prediction task? \n",
    "\n",
    "The researchers pointed out that pre-training with the next sentence prediction task is not really useful and it is not a difficult task to perform as compared to the masked language modeling task. Also, the next sentence prediction task combines both the topic prediction and coherence prediction into a single task. To alleviate this, researchers have introduced the sentence order prediction task. The sentence order prediction is based on the inter-sentence coherence and not on the topic prediction. Let's understand how the sentence prediction task works in detail. \n",
    "\n",
    "## Sentence order prediction\n",
    "Similar to the next sentence prediction task, the sentence order prediction is a binary classification task. In the next sentence prediction task, we train the model to predict whether a sentence pair belongs to the isNext or notNext class whereas, in the sentence order prediction task, we train the model to predict whether a sentence order in a given sentence pair is swapped or not. Let us understand this with an example. Consider the following sentence pair:\n",
    "\n",
    "Sentence 1: She cooked pasta\n",
    "\n",
    "Sentence 2: It was delicious \n",
    "\n",
    "In the given sentence pair, we can understand the sentence 2 is a consecutive sentence of sentence 1 and we label this sentence pair as a positive. Now to create a negative class, we simply swap the sentence order, thus our sentence pair becomes:\n",
    "\n",
    "Sentence 1: It was delicious\n",
    "\n",
    "Sentence 2: She cooked pasta \n",
    "\n",
    "From the preceding sentence pair, we can understand that the sentence order is swapped and we label this sentence pair as a negative. \n",
    "\n",
    "Thus, the sentence order prediction is basically a classification task where the goal of our model is to classify whether the sentence pair belong to a positive class (sentence order not swapped) or a negative class (sentence order swapped). We can now create a dataset for a sentence order prediction task using any monolingual corpus. Say, we have a couple of documents. We simply take two consecutive sentences from a document and label them as positive. Next, we swap the sentences and label them as negative.\n",
    "\n",
    "We learned that the ALBERT model is trained using masked language modeling and the sentence order prediction tasks. But how efficient and powerful our ALBERT model is compared to the BERT? Let's discuss this in the next section. \n",
    "\n",
    "# Comparing ALBERT with BERT \n",
    "\n",
    "Similar to the BERT, the ALBERT is pre-trained with different configurations. In all configurations, ALBERT has few parameters compared to the BERT model. The following table shows the comparison of different configurations of BERT and ALBERT models. We can notice how ALBERT has fewer parameters compared to BERT. For instance, BERT-large has 334M parameters but ALBERT has only 18M parameters: \n",
    "\n",
    "\n",
    "![title](images/2.png)\n",
    "Just like BERT, after pre-training, we can fine-tune the pre-trained ALBERT model on any downstream task. The ALBERT-XXlarge model has significantly outperformed both BERT-base and BERT-large on several language benchmark datasets SQuAD1.1, SQuAD2.0, MNLI SST-2, and RACE.\n",
    "\n",
    "Thus, the ALBERT model can be used as a good alternative to the BERT. In the next section, let's explore how to extract embeddings from the pre-trained ALBERT model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
