{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-lingual language model  \n",
    "\n",
    "In the previous sections, we learned how M-BERT works and we also investigated how multilingual is the M-BERT. We understood that the M-BERT model is pre-trained just like the regular BERT without any specific cross-lingual objective. In this section, let us learn how to pre-train the BERT with the cross-lingual objective. We call the BERT trained with cross-lingual objective a cross-lingual language model (XLM). The XLM model performs better than the M-BERT and it learns the cross-lingual representations. \n",
    "\n",
    "The XLM model is pre-trained using the monolingual and parallel datasets. We know that the monolingual dataset consists of text in many languages. The parallel dataset consists of text in a language pair, that is, it consists of the same text in two different languages. Say, we have an English sentence then we will have a corresponding sentence in another language say, French. We can call this parallel dataset also a cross-lingual dataset. \n",
    "\n",
    "The monolingual dataset is obtained from Wikipedia and the parallel dataset is obtained from several sources which include MultiUN (A Multilingual corpus from United Nation Documents), OPUS ( the open parallel corpus), and IIT Bombay corpus. XLM uses byte pair encoding (BPE) and creates shared vocabulary across all the languages. \n",
    "\n",
    "## Pre-training Strategies \n",
    "The XLM model is pre-trained using the following tasks:\n",
    "\n",
    "- Casual language modeling \n",
    "- Masked language modeling \n",
    "- Translation language modeling \n",
    "Let's take a look at how each of the above tasks works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked language modeling \n",
    "We know that in the masked language modeling (MLM) task, we randomly mask 15% of the tokens and train the model to predict the masked tokens. We mask 15% of tokens with the 80-10-10% rule:\n",
    "\n",
    "- For 80% of the time, we replace a token (word) with [MASK] token\n",
    "- For 10% of the time, we replace a token with a random token (random word)\n",
    "- For 10% of the time, we leave the tokens unchanged\n",
    "\n",
    "We train the XLM using the masked language modeling task just like how we trained the BERT model but with the following two changes:\n",
    "\n",
    "1. We learned that in BERT, we take a sentence pair and randomly mask a few tokens in the sentence pair, and feed the sentence pair as an input. But in XLM, we don't have to feed only the sentence pair, instead, we can feed an arbitrary number of sentences to the model. We keep the total token length to 256.\n",
    "2. To balance the frequent and rare words, we sample tokens according to a multinomial distribution whose weights are proportional to the square root of their invert frequencies. \n",
    "\n",
    "The following figure shows the XLM model with the masked language modeling objective. We can observe that unlike BERT where we feed only a sentence pair here we are feeding an arbitrary number of sentences separated by a special token [/s]. We can also notice that along with token and positional embeddings, here we have language embeddings. Language embeddings are used to represent a language: \n",
    "\n",
    "![title](images/10.png)\n",
    "\n",
    "As shown above, we mask 15% of tokens and train the model to predict the masked token. Next, let's look at another task called translation language modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation language modeling \n",
    "\n",
    "Translation language modeling (TLM) is another interesting pre-training strategy. In casual language modeling and masked language modeling (MLM), we train our model on monolingual data, but in Translation language modeling (TLM), we train the model on cross-lingual data, that is, the parallel data which consists of the same text in two different languages. \n",
    "\n",
    "The TLM method works just like MLM we saw in the previous section. Similar to MLM, here, we train the model to predict the masked word. However, instead of feeding the arbitrary number of sentences, we feed the parallel sentence for learning the cross-lingual representation.\n",
    "\n",
    "The following figure shows the TLM model with the translation language modeling objective. As shown in the following figure, we feed a parallel sentence as an input, that is, the same text in two different languages. In this example, we are feeding the English sentence \"I am a sentence\" along with its French equivalent \"Je suis Ã©tudiant\". We randomly mask a few words in both English and French sentences and feed it to the model as shown below:\n",
    "\n",
    "![title](images/11.png)\n",
    "\n",
    "We train the model to predict the masked token. The model learns to predict the masked token by understanding the context from nearby tokens. Say the model is learning to predict the masked token (word) in an English sentence, then it can not only use the context of the tokens (words) in the English sentence, but it can also use the context of the tokens (words) in the French sentence. This enables the model to align the cross-lingual representation. \n",
    "\n",
    "We can also observe from the preceding figure that language embedding is used to represent different languages and we can notice that we use separate positional embedding for both the sentences. \n",
    "\n",
    "We learned that in XLM, we use three pre-training strategies such as casual language modeling, masked language modeling, and translation language modeling. But how exactly we pre-train the XLM model? Should we use all of these strategies or should we train the model only with a few? Let us find that out in the next section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
