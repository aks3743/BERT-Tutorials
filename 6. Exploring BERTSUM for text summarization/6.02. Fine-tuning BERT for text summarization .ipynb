{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for text summarization \n",
    "\n",
    "In this section, let us understand how to fine-tune the BERT model for performing text summarization. First, we will understand how to fine-tune BERT for extractive summarization, and then we will see how to fine-tune BERT for abstractive summarization. \n",
    "\n",
    "# Extractive summarization using BERT \n",
    "To fine-tune the pre-trained BERT for the extractive summarization task, we slightly modify the input data format of the BERT. Before looking into the modified input data format, first, let us recall how we feed the input data to the BERT. \n",
    "\n",
    "Consider we have two sentences â€“ Paris is a beautiful city. I love Paris. First, we tokenize the sentences and we add [CLS] token only at the beginning of the first sentence and we add [SEP] token at the end of every sentence. Before feeding the tokens to the BERT, we convert them into embedding using three embedding layers known as token embedding, segment embedding, and position embedding. We sum up all the embeddings together element-wise and then we feed them as an input to the BERT. The input data format of BERT is shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/2.png)\n",
    "\n",
    "The BERT model takes this input and returns the representation of every token as an output as shown in the following figure:\n",
    "\n",
    "![title](images/3.png)\n",
    "\n",
    "Now the question is how we can use the BERT for the text summarization task? We know that the BERT model gives the representation of every token. But we don't need a representation of every token. Instead, we need a representation of every sentence. But why?\n",
    "\n",
    "We learned that in extractive summarization, we create a summary by just selecting only the important sentences. We know that a representation of a sentence will hold the meaning of the sentence. If we get a representation of every sentence, then based on the representation, we can decide if the sentence is important or not. If it is important, then we will add it to the summary else we will discard it. Thus, if we obtain the representation of every sentence using BERT, then we can feed the representation to the classifier and the classifier tells us whether the sentence is important or not. \n",
    "\n",
    "Okay, how can we get the representation of a sentence? Can we use the representation of [CLS] token as the representation of the sentence? Yes! But there is a small catch here. We learned that we add the [CLS] token only at the beginning of the first sentence, but in the text summarization task, we feed multiple sentences to the BERT and we need the representation of all the sentences.\n",
    "\n",
    "So, in this case, we modify our input data format to the BERT. We add the [CLS] token at the beginning of every sentence so that we can use the representation of [CLS] token added at the beginning of every sentence as their representation.\n",
    "\n",
    "Consider we have three sentences - sent one, sent two, and sent three. First, we tokenize the sentences and we add [CLS] token at the beginning of every sentence and we also separate each sentence by [SEP] token. The input tokens are shown in the following. As we can observe, we have added the [CLS] token at the beginning of every sentence and we added the [SEP] token at the end of every sentence:\n",
    "\n",
    "Input tokens = [ [CLS], sent, one, [SEP], [CLS], sent, two, [SEP], [CLS], sent, three, [SEP] ]\n",
    "\n",
    "Next, we feed the input tokens to the token, segment, and position embedding layers and convert the input tokens into embeddings. The token embedding layer is shown in the following figure: \n",
    "\n",
    "![title](images/4.png)\n",
    "\n",
    "\n",
    "The next layer is the segment embedding layer. We know that the segment embedding is used to distinguish between the two given sentences. The segment embedding layer returns two embeddings  $E_A$ or  $E_B$. That is, if the input token belongs to sentence A, then the token will be mapped to the embedding  $E_A$ and if the input token belongs to sentence B then the token will be mapped to the embedding $E_B$. But in the text summarization setting, we feed more than two sentences to the BERT. Now how can we map the tokens from more than two sentences to the embedding  $E_A$ and $E_B$?\n",
    "\n",
    "In this case, we use an interval segment embedding. The interval segment embedding is used to distinguish between the multiple given sentences. With internal segment embedding, we map the tokens of the sentence occurring in the odd index to $E_A$  and we map the tokens of the sentence occurring in even index to $E_B$. Say we have 4 sentences, then:\n",
    "\n",
    "- All tokens from sentence 1 will be mapped to $E_A$\n",
    "- All tokens from sentence 2 will be mapped to $E_B$\n",
    "- All tokens from sentence 3 will be mapped to  $E_A$\n",
    "- All tokens from sentence 4 will be mapped to $E_B$\n",
    "\n",
    "The interval segment embedding layer is shown in the following figure. As we can observe from the following image tokens from sentence one are mapped to $E_A$, tokens from sentence two are mapped to $E_B$ and tokens from sentence three are mapped to $E_A$: \n",
    "\n",
    "![title](images/5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next layer is the position embedding layer. The position embedding layer works in the same way as we learned before. The position embedding layer encodes the positional information of every token in the input. The positional embedding layer is shown in the following figure: \n",
    "\n",
    "![title](images/6.png)\n",
    "\n",
    "The final modified input data format with the token, interval segment, and position embedding layer for the extractive summarization task is shown in the following figure: \n",
    "\n",
    "![title](images/7.png)\n",
    "\n",
    "Now, we feed the input to the BERT model with this modified input data format. As shown in the following figure, the BERT model takes this input and returns the representation of every token as an output. Since we added [CLS] token at the beginning of every sentence, we can use the representation of [CLS] token as the sentence representation. As shown in the following figure,  $R_1$ denotes the representation of the sent one, $R_2$ denotes the representation of the sent two, $R_3$  denotes the representation of the sent three. We can call the following BERT model with modified input data format as BERTSUM (BERT for summarization): \n",
    "\n",
    "![title](images/8.png)\n",
    "\n",
    "\n",
    "Note that for obtaining the sentence representation, we don't have to train BERT from scratch. Instead, we can use any pre-trained BERT model but we just need to modify the input data format in a way we discussed earlier so that we can use the representation of every [CLS] token as their corresponding sentence representation. \n",
    "\n",
    "We learned how to obtain the representation of every sentence in the given text using a pre-trained BERT model, but how can we make use of those representations for the extractive summarization task? Let us find that out in the next section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
