{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword tokenization algorithms \n",
    "\n",
    "Subword tokenization is popularly used in many state-of-the-art natural language models such as BERT, GPT-2, etc. It is very effective in handling Out-Of-Vocabulary (OOV) words. In this section, we will understand how subword tokenization works in detail. Before directly looking into the subword tokenization, first, let's take a look into the word level tokenization. \n",
    "\n",
    "Let's suppose we have a training dataset. Now from this training set, we build a vocabulary. To build the vocabulary, we split the text present in the dataset by white space and add all the unique words to the vocabulary. Generally, the vocabulary consists of many words (tokens), but just for a sake of example, let's suppose our vocabulary consists of the following words: \n",
    "\n",
    "vocabulary = [game, the, I, played, walked, enjoy]\n",
    "\n",
    "Now that we created the vocabulary, we use this vocabulary for tokenizing the input. Let's consider an input sentence \"I played the game\". In order to create tokens from the sentence, first, we split the sentence by whitespace and obtain all the words in the sentence. So we have - [I, play, the, game]. Now, we check whether we have all the words (I, play, the, game) present in the vocabulary. Since all the words are present in the vocabulary, our final tokens for the given sentence will be:\n",
    "\n",
    "tokens = [I, played, the, game]\n",
    "\n",
    "Let's consider another sentence: \"I enjoyed the game\". To tokenize the sentence, we split the given sentence by whitespace and obtain the words. Then we have [ I, enjoyed, the, game]. Now, we check whether we have all the words (I, enjoyed, the, game) present in the vocabulary. We can observe that we have all the words present in the vocabulary except for the word enjoyed. Since the word enjoyed is not present in the vocabulary, we replace it with an unknown token <UNK>. Thus, our final tokens will be:\n",
    "\n",
    "tokens = [ I, <UNK>, the, game] \n",
    "\n",
    "We can observe that although we have the word enjoy in our vocabulary, just because we didn't have the exact word enjoyed, it is marked as an unknown word with the <UNK> token. Generally, we build a huge vocabulary with many words, and if a rare word, that is, the word that is not present in the vocabulary comes in, then it will be marked as an unknown word with <UNK> token. But having a huge vocabulary causes memory and performance issues and it still can not handle the unseen words (words that are not present in the vocabulary). \n",
    "\n",
    "Is there a better way to handle this? Yes! This is where subword tokenization helps us. Let's see how subword tokenization works with the same preceding example. In subword tokenization, we split the words into subwords. Say, we split the word played to subwords [play, ed] and the word walked to subwords [walk, ed]. After splitting the subwords we will add it to the vocabulary. Note that our vocabulary consists of only unique words. So, now our vocabulary consists of the following: \n",
    "\n",
    "vocabulary = [game, the, I, play, walk, ed, enjoy]\n",
    "\n",
    "Let's consider the same sentence we saw earlier: \"I enjoyed the game\". To tokenize the sentence, we split the given sentence by whitespace and obtain the words. So we have [ I, enjoyed, the, game]. Now, we check whether we have all the words (I, enjoyed, the, game) present in the vocabulary. We can notice that we have all the words present in the vocabulary except for the word enjoyed. Since the word enjoyed is not present in the vocabulary, we split it into subwords, so we [enjoy, ed]. Now, we check whether we have the words enjoy and ed present in the vocabulary, since they are present in the vocabulary, our tokens become: \n",
    "\n",
    "tokens = [ I, enjoy, ##ed, the, game] \n",
    "\n",
    "We can observe the word ed has two hash signs before the. It indicates that ##ed is a subword and it is preceded by another word. We don't add the ## signs for subword which is at the start of the word and that's why there are no ## signs in the subword enjoy. The ## signs are added just to indicate that it is a subword and it is preceded by another word. In this way subword tokenization handles the unknown words, that is words that are not present in the vocabulary. \n",
    "\n",
    "But the question is, we saw that we split the words played and walked to subwords and added them to the vocabulary. But why we are splitting those words alone? Why not other words in the vocabulary? How do we decide which word to split and which to not? This is where we use the subword tokenization algorithm.\n",
    "\n",
    "Let's learn several interesting subword tokenization algorithms which are used for creating the vocabulary. After creating the vocabulary, we can use it for tokenization. Let us understand the following three popularly used subword tokenization algorithms:\n",
    "\n",
    "- Byte pair encoding\n",
    "- Byte-level Byte pair encoding\n",
    "- WordPiece\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
