{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte pair encoding \n",
    "\n",
    "Let us understand how byte pair encoding works with an example. Let's suppose we have a dataset. First, we extract all the words from the dataset along with their count. Suppose the words extracted from the dataset along with the count are - (cost, 2), (best, 2), (menu, 1), (men, 1), (camel 1). \n",
    "\n",
    "Now, we split all the words into unicode characters and create a character sequence. The following table shows the character sequence along with the word count:\n",
    "\n",
    "\n",
    "![title](images/19.jpg)\n",
    "\n",
    "Next, we define a vocabulary size. Let's suppose, we build a vocabulary of size 14. It implies that we create a vocabulary with 14 tokens. Let us understand how to create the vocabulary using byte pair encoding. \n",
    "\n",
    "First, we add all the unique characters present in the character sequence to the vocabulary as shown below: \n",
    "\n",
    "\n",
    "![title](images/20.png)\n",
    "\n",
    "As we can observe, our vocabulary size is 11. Now, let's see how to add new tokens to the vocabulary.\n",
    "\n",
    "To add a new token to the vocabulary, first, we identify the most frequent symbol pair. Then we merge that most frequent symbol pair and add it to the vocabulary. We repeat this step iteratively until we reach the vocabulary size. Now, let's understand this in detail. \n",
    "\n",
    "By looking at the character sequence below, we can observe that the most frequent symbol pair we have is s and t since the symbol pair s and t have occurred 4 times (2 times in cost snd 2 times in best): \n",
    "\n",
    "![title](images/21.png)\n",
    "\n",
    "So, we merge the symbols s and t and add it to the vocabulary as shown below: \n",
    "\n",
    "![title](images/22.png)\n",
    "\n",
    "Now, we repeat the same step again. That is, again we check for the most frequent symbol pair. We can observe that the most frequent symbol pair we have now is m and e since they have occurred 3 times (1 time in menu, 1 time in men, and 1 time in camel):\n",
    "\n",
    "![title](images/23.png)\n",
    "\n",
    "\n",
    "So, we merge the symbols m and e and add it to the vocabulary as shown below: \n",
    "\n",
    "![title](images/24.png)\n",
    "\n",
    "\n",
    "Again, we check for the most frequent symbol pair. We can observe that the most frequent symbol pair we have now is me and n since they have occurred 2 times. (1 in the menu and 1 in men).\n",
    "\n",
    "![title](images/25.png)\n",
    "\n",
    "\n",
    "So, we merge the symbols me and n and add it to the vocabulary as shown below: \n",
    "\n",
    "![title](images/26.png)\n",
    "\n",
    "In this way, we repeat this step several times until we reach the vocabulary size. From the above figure, we can observe that now our vocabulary has 14 tokens. Since in this example we are creating a vocabulary of size only 18 we stop at this step. \n",
    "\n",
    "Thus, from the given dataset, we built the vocabulary containing 18 tokens: \n",
    "\n",
    "vocabulary = {a,b,c,e,l,m,n,o,s,t,u,st,me,men}\n",
    "\n",
    "The steps involved in the byte pair encoding is given below: \n",
    "\n",
    "1. Extract the words from the given dataset along with their count\n",
    "2. Define the vocabulary size\n",
    "3. Split the words into a character sequence \n",
    "4. Add all the unique characters in our character sequence to the vocabulary \n",
    "5. Select and merge the symbol pair which has a high frequency   \n",
    "6. Repeat step 5 until the vocabulary size is reached \n",
    "\n",
    "Note that while splitting the words to character sequence in step 3, we can also add a special token </w> at the end of the character sequence and treat </w> as a symbol just like other characters. </w> denotes the word boundary. We perform steps (4 to 6) just like we discussed earlier by treating </w> as a symbol. Say a symbol pair e and </w> has occurred most frequently, then we merge them, e</w>, and add it to the vocabulary.  The use of </w> is that it helps us to understand whether a token in the vocabulary belongs to the start of the word or the end of the word. If a token consists of </w> then it means that it belongs to the end of the word. \n",
    "\n",
    "We learned how to build the vocabulary using BPE. Okay, but how to use this vocabulary? We use the vocabulary for tokenizing the given input. Let's understand this with a few examples in the next section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing with BPE \n",
    "In the previous section, we learned that with the given dataset we create the following vocabulary:\n",
    "\n",
    "vocabulary = {a,b,c,e,l,m,n,o,s,t,u,st,me,men}\n",
    "\n",
    "Now, let's see how to use this vocabulary. Let's suppose, our input text consists of only one word - 'mean'. Now we check if the word 'mean' is present in our vocabulary. We can observe it is not present in the vocabulary. So we split the word 'mean' into subwords [me, an]. Now we check if the subwords are present in the vocabulary. We can observe that the subword 'me' is present in the vocabulary but the subword 'an' is not present in our vocabulary. So we split the subword 'an' and now our subwords consist of [me, a,n]. Now we check if the characters 'a' and 'n' are present in our vocabulary. Since they are present in our vocabulary, our final tokens will be:\n",
    "\n",
    "tokens = [me,a,n] \n",
    "\n",
    "Let's consider one more input word: 'bear'. We can observe the word 'bear' is not present in our vocabulary. So now, we split it into subwords [be, ar]. Now we check if the subwords 'be' and 'ar' are present in the vocabulary. The subword 'be' is present but 'ar' is not present in the vocabulary. So we split the subword 'an' and now subwords consist of [be, a,r]. Now we check if the characters 'a' and 'r' are present in the vocabulary. We can notice that 'a' is present in the vocabulary but 'r' is not present in the vocabulary. We can't split any more since now we have only individual characters. Now we replace 'r' with <UNK> token. Thus, our final tokens will be: \n",
    "\n",
    "tokens = [be,a,<UNK>]  \n",
    "\n",
    "Wait. We learned that BPE handles rare words well. But now we have a <UNK> token. This is because since this is a small example, the character 'r' is not present in our vocabulary. But when we create a vocabulary with a huge corpus, our vocabulary will have all the characters.   \n",
    "\n",
    "Let's consider one more input word: 'men'. Now we check if the word 'men' is present in our vocabulary. Since the word 'men' is present in our vocabulary we can directly return it as a token. Thus, our final token will be:\n",
    "\n",
    "tokens = [men]  \n",
    "\n",
    "In this way, we tokenize the input sentence using byte pair encoding. Now that we have understood how Byte pair encoding works in the next section, let us look into Byte-level byte pair encoding. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
