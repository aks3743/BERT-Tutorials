{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In this chapter, we will get started with one of the most popularly used state-of-the-art embedding models called BERT. The BERT has revolutionized the world of NLP by providing state-of-the-art results on many NLP tasks. We will begin the chapter by understanding what BERT is and how it differs from the other embedding models. We will then look into the working of BERT and its configuration in detail.\n",
    "\n",
    "Moving on, we will learn how the BERT model is pre-trained using two tasks called masked language modeling and the next sentence prediction in detail. Then we will look into the pre-training procedure of BERT. At the end of the chapter, we will learn several interesting subword tokenization algorithms such as byte pair encoding, byte-level byte pair encoding, and WordPiece. \n",
    "\n",
    "In this chapter, we will learn the following topics:\n",
    "\n",
    "- Basic idea of BERT \n",
    "- Working of BERT\n",
    "- Configuration of BERT\n",
    "- Pre-training the BERT\n",
    "- Pre-training procedure\n",
    "- Subword tokenization algorithms\n",
    "\n",
    "# Basic idea of BERT \n",
    "\n",
    "BERT stands for Bidirectional Encoder Representation from Transformer. It is the state-of-the-art embedding model published by Google. It has created a major breakthrough in the field of NLP by providing greater results in many NLP tasks such as question-answering, text generation, sentence classification, and many more. One of the major reasons for the success of BERT is that it is a context-based embedding model unlike other popular embedding models like word2vec which are context-free.\n",
    "\n",
    "First, let us understand the difference between the context-based and context-free embedding models. Consider the following two sentences: \n",
    "\n",
    "Sentence A: He got bit by Python \n",
    "\n",
    "Sentence B: Python is my favorite programming language\n",
    "\n",
    "By reading the preceding two sentences, we can understand that the meaning of the word 'Python' is different in both sentences. In sentence A, the word 'Python' refers to the snake while in sentence B, the word 'Python' refers to the programming language. \n",
    "\n",
    "Now, if we get embeddings for the word 'Python' in the above two sentences using an embedding model like word2vec, the embedding of the word 'Python' would be the same in both the sentence and thus it makes the meaning of the word 'Python' the same in both the sentence. This is because word2vec is the context-free model, so, it will ignore the context and always give the same embedding for the word 'Python' irrespective of the context. \n",
    "\n",
    "BERT on the other hand is a context-based model. It will understand the context and then generate the embedding for the word based on the context. So, for the above two sentences, it will give different embeddings for the word 'Python' based on the context. But how do this works? How does BERT understand the context? Let's explore more. \n",
    "\n",
    "Let's take Sentence A: He got bit by Python. First, the BERT relates each word in the sentence to all the words in the sentence to understand the contextual meaning of every word. So, to understand the contextual meaning of the word 'Python', the BERT takes the word 'Python' and relates it to all the words in the sentence. By doing this, BERT can understand that the word 'Python' in sentence A denotes the snake using the word bit as shown below:\n",
    "\n",
    "\n",
    "![title](images/1.png)\n",
    "\n",
    "Now, let's take the Sentence B: Python is my favorite programming language. Similarly, here the BERT relates each word in the sentence to all the words in the sentence to understand the contextual meaning of every word. So, the BERT takes the word 'Python' and relates it to all the words in the sentence to understand the meaning of the word 'Python'. By doing this, BERT understands that the word 'Python' in sentence B is related to a programming language using the word programming as shown below:\n",
    "\n",
    "![title](images/2.png)\n",
    "Thus, unlike context-free models like word2vec that generate static embeddings irrespective of the context, BERT generates dynamic embedding based on the context. \n",
    "\n",
    "Okay, the question is how exactly does the BERT works? How does it understand the context? Now that we have a basic idea of BERT, in the next section, we will explore more about BERT and find answers to these questions. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
