{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# WordPiece\n",
    "WordPiece works similar to byte pair encoding (BPE) with a small difference. We learned that in BPE, from the given data, first we extract the words with their count. Then we split the words into character sequences. Next, we merge the symbol pair which has a high frequency. We keep merging symbol pair which has high frequency iteratively until we reach the vocabulary size. We do the same in WordPiece, but one difference is that here we don't merge symbol pair based on the frequency instead we merge the symbol based on the likelihood. So, we merge the symbol pair which has a high likelihood of the language model which is trained on the given training data. Let's understand this with an example. \n",
    "\n",
    "Consider the same example we learned in the BPE section:\n",
    "\n",
    "![title](images/27.png)\n",
    "\n",
    "\n",
    "We learned that in BPE we merge the most frequent symbol pair. In BPE we merged the symbol pair s and t since they have occurred 4 times. But now in this method, we don't merge symbol pair based on frequency instead we merge on them based on the likelihood. So first we check the likelihood of the language model trained on the trained data for every symbol pair, then we merge the one which has the highest likelihood. The likelihood of the symbol pair s and t is computed as shown below: \n",
    "\n",
    "$$\\frac{p(st)}{p(s) p(t)} $$\n",
    "\n",
    "If the likelihood is high, we simply merge the symbol pair to the vocabulary. In this way compute the likelihood of all symbol pairs and merge the one which has the maximum likelihood. The below steps helps us to understand this better: \n",
    "\n",
    "1. Extract the words from the given dataset along with their count\n",
    "2. Define the vocabulary size\n",
    "3. Split the words into a character sequence \n",
    "4. Add all the unique characters in our character sequence to the vocabulary \n",
    "5. Build the language model on the training set \n",
    "6. Select and merge the symbol pair which has the maximum likelihood of the language model trained on the training set  \n",
    "7. Repeat step 6 until the vocabulary size is reached \n",
    "\n",
    "After building the vocabulary, we use it for tokenization. Let's suppose, the following is the vocabulary we build using the WordPiece method:\n",
    "\n",
    "vocabulary = {a,b,c,e,l,m,n,o,s,t,u,st,me}\n",
    "\n",
    "Let's suppose, our input text consists of only one word -'stem'. We can observe the word 'stem' is not present in our vocabulary. So now, we split it into subwords [st, ##em]. Now we check if the subwords 'st' and 'em' are present in the vocabulary. The subword 'st' is present but 'em' is not present in the vocabulary. So we split the subword 'em' and now subwords consist of [be, ##e,##m]. Now we check if the characters 'e' and 'm' are present in the vocabulary. Since they are present in the vocabulary, our final tokens will be: \n",
    "\n",
    "tokens = [st, ##e, ##m] \n",
    "\n",
    "In this chapter, we learned how BERT is pre-trained in detail and we also looked into the different tokenization techniques, in this next chapter, let's understand how to apply the pre-trained BERT in detail. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
