{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of BERT \n",
    "\n",
    "The researchers of BERT have presented the model in two standard configurations:\n",
    "\n",
    "BERT-base\n",
    "BERT-large\n",
    "Let's take a look at each one of them. \n",
    "\n",
    "## BERT-base\n",
    "\n",
    "BERT-base consists of 12 encoder layers each stacked one above the other. All the encoders use 12 attention heads. The feedforward network in the encoder consists of 768 hidden units. Thus, the size of the representation obtained from the BERT-base will be 768. \n",
    "\n",
    "We use the following notations: \n",
    "\n",
    "- Number of encoder layers is denoted by  $L$\n",
    "- Attention head is denoted by $A$\n",
    "- Hidden unit is denoted by $H$\n",
    "\n",
    "Thus, in the BERT-base model, we have, $L=12, A=12, H=768$. The total number of parameters in BERT-base is 110 million. The BERT-base model is shown in the following figure:\n",
    "\n",
    "\n",
    "![title](images/5.png)\n",
    "\n",
    "## BERT-large\n",
    "\n",
    "BERT-large consists of 24 encoder layers each stacked one above the other. All the encoders use 16 attention heads. The feedforward network in the encoder consists of 1024 hidden units. Thus, the size of the representation obtained from the BERT-large will be 1024. \n",
    "\n",
    "Thus, in the BERT-base model, we have, $L=24, A=16, H=1024$. The total number of parameters in BERT-base is 340 million. The BERT-large model is shown in the following figure:\n",
    "\n",
    "\n",
    "![title](images/6.png)\n",
    "\n",
    "# Other configurations of BERT\n",
    "Apart from the preceding two standard configurations, we can also build the BERT model with other different configurations. Some of the smaller configurations of the BERT are shown in the following: \n",
    "\n",
    "- Bert-tiny with $L=2, H=128$\n",
    "- Bert -mini with $L=4, H=256$\n",
    "- Bert-small with $L=4, H=512$\n",
    "- Bert-medium with $L=8, H=512$\n",
    "\n",
    "The following figure shows the different configurations of BERT:\n",
    "\n",
    "\n",
    "![title](images/7.png)\n",
    "We can use smaller configurations of BERT in settings where the computational resource is limited. However, the standard BERT configurations such as BERT-base and BERT-large give more accurate results and they are most popularly used.  \n",
    "\n",
    "Okay, we understood how BERT works and we also saw the different configurations of BERT. But how to train the BERT to generate representations? What data we should use? what is the training strategy we should follow? This is exactly what we discuss in the next section. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
