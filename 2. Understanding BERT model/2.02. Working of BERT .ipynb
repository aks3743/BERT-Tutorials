{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Working of BERT \n",
    "\n",
    "\n",
    "Bidirectional Encoder Representation from Transformer (BERT) as the name suggests, BERT is basically the transformer model. We can perceive BERT as the transformer but only with the encoder. \n",
    "\n",
    "Remember, in the previous chapter \"A guide to transformer model\", we learned that, we feed the sentence as an input to the transformer's encoder and it returns the representation for each word in the sentence as an output? Well, that's exactly what the BERT is, Encoder Representation from Transformer. Okay, what about the term Bidirectional? \n",
    "\n",
    "The encoder of the transformer is bidirectional in nature since they can read a sentence in both directions. Thus, BERT is basically the Bidirectional Encoder Representation obtained from the Transformer.\n",
    "\n",
    "Let's understand how BERT is bidirectional encoder representation from the transformer with an example. Let's take the same sentences we saw in the previous section.\n",
    "\n",
    "Say, we have a sentence A: 'He got bit by Python'.  Now, we feed this sentence as an input to the transformer's encoder and get the contextual representation (embedding) of each word in the sentence as an output. Once, we feed the sentence as an input to the encoder, the encoder understands the context of the each word in the sentence using the multi-head attention mechanism (relates each word in the sentence to all the words in the sentence to learn the relationship and contextual meaning of words) and returns the contextual representation of each word in the sentence as an output.\n",
    "\n",
    "As shown in the following figure, we feed the sentence as an input to the transformer's encoder and get the representation of each word in the sentence as an output. We can stack up N number of encoders as shown in the below figure, we have expanded only the encoder 1 to reduce the clutter. In the below figure, $R_{\\text{he}}$  denotes the representation of the word 'He', $R_{\\text{got}}$ is the representation of the word 'got' and so on. The representation size of each token will be the size of the encoder layer. Suppose, the size of the encoder layer is 768, then the representation size of each token will be 768:\n",
    "\n",
    "\n",
    "![title](images/3.png)\n",
    "\n",
    "\n",
    "Similarly, if we feed the sentence B - 'Python is my favorite programming language' to the transformer's encoder, we get the contextual representation of each word in the sentence as an output as shown below:\n",
    "\n",
    "\n",
    "\n",
    "![title](images/4.png)\n",
    "\n",
    "Thus with the BERT model, for a given sentence, we obtain the contextual representation (embedding) of each word in the sentence as an output. Now that we understood how BERT generates contextual representation, in the next section, we will look into different configurations of BERT. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
