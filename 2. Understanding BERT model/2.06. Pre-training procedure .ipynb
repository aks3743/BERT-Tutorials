{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training procedure \n",
    "\n",
    "BERT is pre-trained using Toronto BookCorupus and Wikipedia dataset. We learned that BERT is pre-trained using Masked language modeling (cloze task) and the next sentence prediction task. Now, how to prepare the dataset to train BERT using these two tasks? \n",
    "\n",
    "First, we sample two sentences (two text spans) from the corpus. Say, we sampled two sentences A and B. The sum of the total number of tokens from the two sentences A and B should be less than or equal to 512. While sampling two sentences (two text spans), for 50% of the time, we sample the sentence B as a next sentence of sentence A, and for the other 50% of the time, we sentence B  as not the next sentence of sentence A. \n",
    "\n",
    "Suppose, we sampled the following two sentences:\n",
    "\n",
    "Sentence A: We enjoyed the game \n",
    "\n",
    "Sentence B: Turn the radio on\n",
    "\n",
    "First, we tokenize the sentence using WordPiece tokenizer, add the [CLS] token to the beginning of the first sentence, and the [SEP] token at the end of every sentence. So our tokens become the following:\n",
    "\n",
    "tokens = [ [CLS], we, enjoyed, the, game, [SEP], turn, the radio, on, [SEP] ]\n",
    "\n",
    "Next, we randomly mask 15% of tokens according to 80-10-10% rule. Suppose we masked the token 'game', then we have: \n",
    "\n",
    "tokens = [ [CLS], we, enjoyed, the, [MASK], [SEP], turn, the radio, on, [SEP] ]\n",
    "\n",
    "Now, we feed the tokens to BERT and train the BERT in predicting the masked tokens and also to classify whether the sentence B is the next sentence of sentence A. That is, we train the BERT with both the MLM and NSP tasks simultaneously. \n",
    "\n",
    "BERT is trained on 256 sequences per batch for 1,000,000 steps. We use Adam optimizer for training with learning rate, $lr=1e-4$, $\\beta_1 = 0.9,\\beta_2 = 0.99$  and with warmup steps set to 10,000. Wait, what are warmup steps?\n",
    "\n",
    "During training, we can take large steps in the initial iterations by setting a high learning rate but we should take smaller steps in the later iterations by setting a low learning rate. This is because in the initial iteration, we will be far from convergence, so it is okay to take large steps, but in the later iterations, we will be close to convergence and in that case, if we take a larger step, we miss out the convergence. Setting the value of learning rate high in the initial iterations and then decreasing it in the later iterations is known as learning rate scheduling. \n",
    "\n",
    "The warmup step is used in learning rate scheduling. Say our learning rate is 1e-4 and the warmup step is 10,000. It implies that we increase the learning rate linearly from 0 to 1e-4 in the initial 10,000 iterations. But after the 10,000 iterations, we decrease the learning rate linearly as we move close to the convergence. \n",
    "\n",
    "We also apply dropout on all layers with 0.1 dropout probability. BERT uses an activation function called GELU. The GELU stands for Gaussian Error Linear Unit. \n",
    "\n",
    "The GELU function is given as: \n",
    "\n",
    "$$\\text{GELU}(x) = x \\Phi (x) $$\n",
    "\n",
    "Where  $\\phi(x)$ is the standard Gaussian cumulative distribution function. The GELU function is approximated as given below: \n",
    "\n",
    "$$\\text{GELU}(x) = 0.5 x (1 + \\text{tanh}[\\sqrt{2/\\pi}   (x+0.044715x^3)]) $$\n",
    "\n",
    "The below figure shows the plot of GELU function:\n",
    "\n",
    "![title](images/18.png)\n",
    "\n",
    "\n",
    "That's it. In this way, we can pre-train the BERT using MLM and NSP tasks. The pre-trained BERT model can be used for a variety of tasks and we will learn how to use the pre-trained BERT in detail in the next chapter. In the next section, we will learn several interesting subword tokenization algorithms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
